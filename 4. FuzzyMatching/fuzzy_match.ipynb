{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for child and direct matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_calculator(Dbase_df,Ass_df,start_list):\n",
    "    dict_df_80_90 = {}\n",
    "    dict_df_90_100 = {}\n",
    "    for i in start_list:\n",
    "        db =Dbase_df.query('Start_Letter ==@i')\n",
    "        print(i)\n",
    "        Ass = Ass_df.query('Start_Letter ==@i')\n",
    "        combo_df = pd.merge(Ass,db, on ='Start_Letter')\n",
    "        combo_df['Score']=combo_df.apply(lambda x:fuzz.token_set_ratio(x.processed_name,x.DB_processed_name),axis=1)\n",
    "        dict_df_80_90[i] = combo_df.query('80 < Score <= 90')\n",
    "        dict_df_90_100[i] = combo_df.query('Score > 90')\n",
    "    df_90_100 = pd.concat([pd.concat([v]) for k,v in dict_df_90_100.items()])\n",
    "    df_80_90 = pd.concat([pd.concat([v]) for k,v in dict_df_80_90.items()])\n",
    "    return df_90_100,df_80_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_calculator_v1(Dbase_df,Ass_df,start_list):\n",
    "    dict_df_others = {}\n",
    "    dict_df_95_100 = {}\n",
    "    for i in start_list:\n",
    "        db =Dbase_df.query('Start_Letter ==@i')\n",
    "        print(i)\n",
    "        Ass = Ass_df.query('Start_Letter ==@i')\n",
    "        combo_df = pd.merge(Ass,db, on ='Start_Letter')\n",
    "        combo_df['Score']=combo_df.apply(lambda x:fuzz.token_set_ratio(x.processed_name,x.DB_processed_name),axis=1)\n",
    "        dict_df_95_100[i] = combo_df.query('Score >= 95')\n",
    "        dict_df_others[i] = combo_df.query('90 >=Score < 95')\n",
    "        \n",
    "    df_95_100 = pd.concat([pd.concat([v]) for k,v in dict_df_95_100.items()])\n",
    "    df_others = pd.concat([pd.concat([v]) for k,v in dict_df_others.items()])\n",
    "    return df_95_100,df_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the direct matches\n",
    "Ass_EMEA_focus = pd.read_csv('Focus8k_ind.csv')\n",
    "Dbase_direct = pd.read_csv('mn_db_names_direct.csv')\n",
    "\n",
    "Dbase_direct['DB_processed_name'] = Dbase_direct.demandbase_company_name.str.lower()\n",
    "Dbase_direct['length'] = Dbase_direct.DB_processed_name.str.len()\n",
    "Dbase_direct = Dbase_direct[Dbase_direct.length > 1][['demandbase_sid','demandbase_company_name','DB_processed_name']]\n",
    "\n",
    "Ass_EMEA_focus['processed_name'] = Ass_EMEA_focus.sub_name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Letters in Account Segment Summary\n",
      " ['a' 's' 'r' 'j' 'l' 'f' 'd' 'c' 'b' 't' 'n' 'k' 'g' 'e' 'x' 'p' 'i' 'o'\n",
      " 'y' 'm' 'h' 'z' 'w' 'v' 'q' 'u' 'é' '3' '2' '1' '4' '@' '�' '5' '6' '9'\n",
      " '8']\n",
      "Starting Letters in Demand Base\n",
      " ['m' 'b' 'a' 'c' 't' 'l' 'i' 'p' 'r' 'g' 'u' 's' 'h' 'f' 'w' 'n' 'z' 'j'\n",
      " 'd' 'e' 'o' 'v' 'k' 'q' 'x' 'y' '1' '3' '2' '4' '8' '5' 'é' '9' '@' '6']\n"
     ]
    }
   ],
   "source": [
    "Dbase_direct['Start_Letter'] = Dbase_direct.DB_processed_name.str[0].str.lower()\n",
    "Ass_EMEA_focus['Start_Letter'] = Ass_EMEA_focus.processed_name.str[0].str.lower()\n",
    "start_list_Ass = Ass_EMEA_focus.Start_Letter.unique()\n",
    "print (\"Starting Letters in Account Segment Summary\\n\",start_list_Ass)\n",
    "Dbase_direct = Dbase_direct.loc[Dbase_direct['Start_Letter'].isin(start_list_Ass)]\n",
    "start_list_db = Dbase_direct.Start_Letter.unique()\n",
    "print (\"Starting Letters in Demand Base\\n\",start_list_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_df_100 = {}\n",
    "for i in start_list_db:\n",
    "        db =Dbase_direct.query('Start_Letter ==@i')\n",
    "        print(i)\n",
    "        Ass = Ass_EMEA_focus.query('Start_Letter ==@i')\n",
    "        combo_df = pd.merge(Ass,db, on ='Start_Letter')\n",
    "        combo_df['Score'] = np.where(combo_df.processed_name == combo_df.DB_processed_name ,100,0)\n",
    "        dict_df_100[i] = combo_df.query('Score == 100')\n",
    "df_100 = pd.concat([pd.concat([v]) for k,v in dict_df_100.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_100.to_csv('Output/df_100_direct_matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the direct matches\n",
    "df_100 = pd.read_csv('Output/df_100_direct_matches.csv')\n",
    "df_100_dmd_sids = np.array(df_100.demandbase_sid.unique())\n",
    "df_child_input =Dbase_direct [~Dbase_direct['demandbase_sid'].isin(df_100_dmd_sids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading the child keys file\n",
    "Ass_Focus_Terr_Child = pd.read_csv('Child_de_fr_uk.csv')\n",
    "Ass_Focus_Terr_Child['processed_name'] = Ass_Focus_Terr_Child.child_name.str.lower()\n",
    "Ass_Focus_Terr_Child = Ass_Focus_Terr_Child[['sub_std_name_key','child_name','child_key','processed_name']]\n",
    "Ass_Focus_Terr_Child['Start_Letter'] = Ass_Focus_Terr_Child.processed_name.str[0].str.lower()\n",
    "Ass_Focus_Terr_Child = Ass_Focus_Terr_Child.sort_values(['Start_Letter'],ascending = True)\n",
    "df_child_start_list = np.array(df_child_input.Start_Letter.unique())\n",
    "Ass_Focus_Terr_Child = Ass_Focus_Terr_Child.loc[Ass_Focus_Terr_Child['Start_Letter'].isin(df_child_start_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "dict_df_100_child = {}\n",
    "for i in df_child_start_list:\n",
    "        db =df_child_input.query('Start_Letter ==@i')\n",
    "        print(i)\n",
    "        Ass = Ass_Focus_Terr_Child.query('Start_Letter ==@i')\n",
    "        combo_df = pd.merge(Ass,db, on ='Start_Letter')\n",
    "        combo_df['Score'] = np.where(combo_df.processed_name == combo_df.DB_processed_name ,100,0)\n",
    "        dict_df_100_child[i] = combo_df.query('Score == 100')\n",
    "df_100_child = pd.concat([pd.concat([v]) for k,v in dict_df_100_child.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_100_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DONT RUN THE BELOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ass_EMEA_focus = pd.read_csv('Focus8k_ind.csv')\n",
    "Dbase = pd.read_csv('mn_db_names_1.csv')\n",
    "pre_mapped_dbs = pd.read_excel('Output/showcase.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_mapped_dbs_list = np.array(pre_mapped_dbs.demandbase_sid.unique())\n",
    "Dbase = Dbase.loc[~Dbase['demandbase_sid'].isin(pre_mapped_dbs_list)]\n",
    "Dbase= Dbase.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "['processed_nAss_EMEA_focusame'] = Ass_EMEA_focus.sub_name.str.replace('\\W',' ')\n",
    "Ass_EMEA_focus['processed_name'].replace(to_replace=\"[^\\x00-\\x7F]+\", value=r\" \", regex=True, inplace=True)\n",
    "Ass_EMEA_focus ['processed_name']=Ass_EMEA_focus.processed_name.str.strip()\n",
    "Ass_EMEA_focus['length'] = Ass_EMEA_focus.processed_name.str.len()\n",
    "Ass_EMEA_focus = Ass_EMEA_focus[Ass_EMEA_focus.length > 1][['sub_std_name_key','sub_name','processed_name','tap_industry']]\n",
    "\n",
    "Dbase['DB_processed_name']= Dbase.demandbase_company_name.str.replace('\\W',' ')\n",
    "Dbase['DB_processed_name'].replace(to_replace=\"[^\\x00-\\x7F]+\", value=r\" \", regex=True, inplace=True)\n",
    "Dbase ['DB_processed_name']=Dbase.DB_processed_name.str.strip()\n",
    "Dbase['length'] = Dbase.DB_processed_name.str.len()\n",
    "Dbase = Dbase[Dbase.length > 1][['demandbase_sid','demandbase_company_name','DB_processed_name']]\n",
    "\n",
    "Dbase['Start_Letter'] = Dbase.DB_processed_name.str[0].str.lower()\n",
    "Ass_EMEA_focus['Start_Letter'] = Ass_EMEA_focus.processed_name.str[0].str.lower()\n",
    "Dbase =Dbase.sort_values(['Start_Letter'],ascending = True)\n",
    "Ass_EMEA_focus = Ass_EMEA_focus.sort_values(['Start_Letter'],ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_list_Ass = Ass_EMEA_focus.Start_Letter.unique()\n",
    "print (\"Starting Letters in Account Segment Summary\\n\",start_list_Ass)\n",
    "Dbase = Dbase.loc[Dbase['Start_Letter'].isin(start_list_Ass)]\n",
    "start_list_db = Dbase.Start_Letter.unique()\n",
    "print (\"Starting Letters in Demand Base\\n\",start_list_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Start_count_grouped = Dbase.groupby(['Start_Letter'])['DB_processed_name'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calling the scoring function and writing the output to pickle files\n",
    "df_stage1_90_100,df_stage1_80_90= score_calculator(Dbase,Ass_EMEA_focus,start_list_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage1_90_100.to_pickle('Output/df_stage1_90_100.pkl')\n",
    "df_stage1_80_90.to_pickle('Output/df_stage1_80_90.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage1_90_100.to_excel('Output/df_stage1_90_100.xls')\n",
    "df_stage1_80_90.to_excel('Output/df_stage1_80_90.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_stage1_95_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Processed File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage1_90_100 = pd.read_excel('Output/df_stage1_90_100.xls')\n",
    "df_stage1_80_90 = pd.read_excel('Output/df_stage1_80_90.xls')\n",
    "dict_1 = pd.read_csv('dictionary.csv')\n",
    "frames = [df_stage1_90_100,df_stage1_80_90]\n",
    "working_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df.processed_name =working_df.processed_name+' '\n",
    "working_df.DB_processed_name =working_df.DB_processed_name+' '\n",
    "removal_list =list(dict_1.dictionary)\n",
    "for idx,val in enumerate(removal_list):\n",
    "    pat1= '|'.join([val])\n",
    "    working_df.processed_name =working_df.processed_name.str.lower().replace(pat1,' , ',regex= True)\n",
    "    working_df.DB_processed_name =working_df.DB_processed_name.str.lower().replace(pat1,' , ',regex= True)\n",
    "working_df['dict_Score']=working_df.apply(lambda x:fuzz.token_set_ratio(x.processed_name,x.DB_processed_name),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df.to_excel('Output/df_stage2_All_mn.xls')\n",
    "df_Stage2_abridged_90_100 = working_df.query('dict_Score >=90')\n",
    "df_Stage2_abridged_90_100.to_excel('Output/df_Stage2_abridged_90_100_mn.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Stage2_abridged_90_100 = pd.read_excel('Output/df_Stage2_abridged_90_100_mn.xls')\n",
    "AEC_Stage_1_Final = pd.read_excel('AEC_Stage_1_Final.xlsx')\n",
    "dbase_list_AEC = AEC_Stage_1_Final.demandbase_sid.unique()\n",
    "df_Stage2_abridged_90_100_deduped =df_Stage2_abridged_90_100 [~df_Stage2_abridged_90_100['demandbase_sid'].isin(dbase_list_AEC)]\n",
    "df_Stage2_abridged_90_100_deduped.to_excel('Output/Final.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WORK STOPPED HERE--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Stage2_90_100.sub_std_name_key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage1_85_95 = pd.read_pickle('Output/df_stage1_85_95.pkl')\n",
    "df_stage1_75_85 = pd.read_pickle('Output/df_stage1_75_85.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage1_matched = df_stage1_95_100_flagged[df_stage1_95_100_flagged.Match == 1]\n",
    "df_stage1_unmatched = df_stage1_95_100_flagged[df_stage1_95_100_flagged.Match == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine Unmatched records and the records with scores 85-95\n",
    "frames1 = [df_stage1_unmatched[['sub_std_name_key', 'sub_name', 'processed_name', 'tap_industry',\n",
    "       'Start_Letter', 'demandbase_sid', 'demandbase_company_name',\n",
    "       'DB_processed_name']],df_stage1_85_95[['sub_std_name_key', 'sub_name', 'processed_name', 'tap_industry',\n",
    "       'Start_Letter', 'demandbase_sid', 'demandbase_company_name',\n",
    "       'DB_processed_name']]]\n",
    "df_stage2_input = pd.concat(frames1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove matched demandbase_sid's from all further processing\n",
    "matched_db_list = np.array(df_stage1_matched.demandbase_sid.unique())\n",
    "df_stage2_input = df_stage2_input.loc[~df_stage2_input['demandbase_sid'].isin(matched_db_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_df = df_stage2_input[['demandbase_sid','demandbase_company_name','Start_Letter','DB_processed_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the child keys file\n",
    "Ass_Focus_Terr_Child = pd.read_csv('Child_de_fr_uk.csv')\n",
    "Ass_Focus_Terr_Child['processed_name'] = Ass_Focus_Terr_Child.child_name.str.replace('\\W',' ')\n",
    "Ass_Focus_Terr_Child['processed_name'].replace(to_replace=\"[^\\x00-\\x7F]+\", value=r\" \", regex=True, inplace=True)\n",
    "Ass_Focus_Terr_Child ['processed_name']=Ass_Focus_Terr_Child.processed_name.str.strip()\n",
    "Ass_Focus_Terr_Child['length'] = Ass_Focus_Terr_Child.processed_name.str.len()\n",
    "Ass_Focus_Terr_Child = Ass_Focus_Terr_Child[Ass_Focus_Terr_Child.length > 1][['sub_std_name_key','child_name','child_key','processed_name']]\n",
    "Ass_Focus_Terr_Child['Start_Letter'] = Ass_Focus_Terr_Child.processed_name.str[0].str.lower()\n",
    "Ass_Focus_Terr_Child = Ass_Focus_Terr_Child.sort_values(['Start_Letter'],ascending = True)\n",
    "working_df_Start_list = np.array(working_df.Start_Letter.unique())\n",
    "Ass_Focus_Terr_Child = Ass_Focus_Terr_Child.loc[Ass_Focus_Terr_Child['Start_Letter'].isin(working_df_Start_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ass_Focus_Terr_Child.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage2_95_100,df_stage2_90_95 = score_calculator_v1(working_df,Ass_Focus_Terr_Child,working_df_Start_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stage2_95_100.to_pickle('Output/df_stage2_95_100.pkl')\n",
    "df_stage2_90_95.to_pickle('Output/df_stage2_90_95.pkl')\n",
    "df_stage2_95_100[0:456].to_excel('Output/df_stage2_95_100_1.xls')\n",
    "df_stage2_95_100[457:].to_excel('Output/df_stage2_95_100_2.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stage2_95_100.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
